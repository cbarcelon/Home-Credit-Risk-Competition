{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['application_test.csv', 'application_train.csv', 'bureau.csv', 'bureau_balance.csv', 'Cleaned_Aggregated_Features.csv', 'clean_manual.csv', 'clean_manual.csv.zip', 'credit_card_balance.csv', 'home-credit-default-risk-feature-tools.zip', 'HomeCredit_columns_description.csv', 'installments_payments.csv', 'POS_CASH_balance.csv', 'previous_application.csv', 'sample_submission.csv', 'test_app_bureau.csv', 'test_app_previous.csv', 'train_app_bureau.csv', 'train_app_previous.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('../Input/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../Input/train_app_bureau.csv')\n",
    "test_df = pd.read_csv('../Input/test_app_bureau.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 columns were label encoded.\n"
     ]
    }
   ],
   "source": [
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in train_df:\n",
    "    if train_df[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(train_df[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(train_df[col])\n",
    "            # Transform both training and testing data\n",
    "            train_df[col] = le.transform(train_df[col])\n",
    "            test_df[col] = le.transform(test_df[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape:  (307511, 455)\n",
      "Testing Features shape:  (48744, 451)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding of categorical variables\n",
    "train_df = pd.get_dummies(train_df)\n",
    "test_df = pd.get_dummies(test_df)\n",
    "\n",
    "print('Training Features shape: ', train_df.shape)\n",
    "print('Testing Features shape: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape:  (307511, 452)\n",
      "Testing Features shape:  (48744, 451)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_df['TARGET']\n",
    "\n",
    "# Align the training and testing data, keep only columns present in both dataframes\n",
    "train_df, test_df = train_df.align(test_df, join = 'inner', axis = 1)\n",
    "\n",
    "# Add the target back in\n",
    "train_df['TARGET'] = train_labels\n",
    "\n",
    "print('Training Features shape: ', train_df.shape)\n",
    "print('Testing Features shape: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (307511, 451)\n",
      "Testing data shape:  (48744, 451)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Drop the target from the training data\n",
    "if 'TARGET' in train_df:\n",
    "    train = train_df.drop(columns = ['TARGET'])\n",
    "else:\n",
    "    train = train_df.copy()\n",
    "    \n",
    "# Feature names\n",
    "features = list(train.columns)\n",
    "\n",
    "# Copy of the testing data\n",
    "#test = test_df.copy()\n",
    "\n",
    "# Median impuation of missing values\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "# Scale each feature to 0-1\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "# Fit on the traing data\n",
    "imputer.fit(train)\n",
    "\n",
    "# Transform both the training and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test_df)\n",
    "\n",
    "# Repeat with the scaler\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "del train_df, test_df\n",
    "print('Training data shape: ', train.shape)\n",
    "print('Testing data shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NbsvmClassifier(C=1, dual=True, n_jobs = -1)\n",
    "clf.fit(train, train_labels)\n",
    "preds = clf.predit_proba(test)[:,1]\n",
    "\n",
    "# Submission dataframe\n",
    "submit = test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = preds\n",
    "# Save the submission to csv file\n",
    "submit.to_csv('../Submissions/NBSVM_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
