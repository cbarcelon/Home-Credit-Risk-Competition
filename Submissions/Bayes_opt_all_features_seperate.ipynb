{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bayesian-optimization\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from scipy.stats import ranksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(data, verbose = True):\n",
    "    start_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "\n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_feature_with_target(feature, target):\n",
    "    c0 = feature[target == 0].dropna()\n",
    "    c1 = feature[target == 1].dropna()\n",
    "        \n",
    "    if set(feature.unique()) == set([0, 1]):\n",
    "        diff = abs(c0.mean(axis = 0) - c1.mean(axis = 0))\n",
    "    else:\n",
    "        diff = abs(c0.median(axis = 0) - c1.median(axis = 0))\n",
    "        \n",
    "    p = ranksums(c0, c1)[1] if ((len(c0) >= 20) & (len(c1) >= 20)) else 2\n",
    "        \n",
    "    return [diff, p]\n",
    "\n",
    "def clean_data(data):\n",
    "    warnings.simplefilter(action = 'ignore')\n",
    "    \n",
    "    # Removing empty features\n",
    "    nun = data.nunique()\n",
    "    empty = list(nun[nun <= 1].index)\n",
    "    \n",
    "    data.drop(empty, axis = 1, inplace = True)\n",
    "    print('After removing empty features there are {0:d} features'.format(data.shape[1]))\n",
    "    # Removing features with the same distribution on 0 and 1 classes\n",
    "    corr = pd.DataFrame(index = ['diff', 'p'])\n",
    "    ind = data[data['TARGET'].notnull()].index\n",
    "    \n",
    "    for c in data.columns.drop('TARGET'):\n",
    "        corr[c] = corr_feature_with_target(data.loc[ind, c], data.loc[ind, 'TARGET'])\n",
    "\n",
    "    corr = corr.T\n",
    "    corr['diff_norm'] = abs(corr['diff'] / data.mean(axis = 0))\n",
    "    \n",
    "    to_del_1 = corr[((corr['diff'] == 0) & (corr['p'] > .05))].index\n",
    "    to_del_2 = corr[((corr['diff_norm'] < .5) & (corr['p'] > .05))].drop(to_del_1).index\n",
    "    to_del = list(to_del_1) + list(to_del_2)\n",
    "    if 'SK_ID_CURR' in to_del:\n",
    "        to_del.remove('SK_ID_CURR')\n",
    "        \n",
    "    data.drop(to_del, axis = 1, inplace = True)\n",
    "    print('After removing features with the same distribution on 0 and 1 classes there are {0:d} features'.format(data.shape[1]))\n",
    "    \n",
    "    # Removing features with not the same distribution on train and test datasets\n",
    "    corr_test = pd.DataFrame(index = ['diff', 'p'])\n",
    "    target = data['TARGET'].notnull().astype(int)\n",
    "    \n",
    "    for c in data.columns.drop('TARGET'):\n",
    "        corr_test[c] = corr_feature_with_target(data[c], target)\n",
    "\n",
    "    corr_test = corr_test.T\n",
    "    corr_test['diff_norm'] = abs(corr_test['diff'] / data.mean(axis = 0))\n",
    "    \n",
    "    bad_features = corr_test[((corr_test['p'] < .05) & (corr_test['diff_norm'] > 1))].index\n",
    "    bad_features = corr.loc[bad_features][corr['diff_norm'] == 0].index\n",
    "    \n",
    "    data.drop(bad_features, axis = 1, inplace = True)\n",
    "    print('After removing features with not the same distribution on train and test datasets there are {0:d} features'.format(data.shape[1]))\n",
    "    \n",
    "    del corr, corr_test\n",
    "    gc.collect()\n",
    "    \n",
    "    # Removing features not interesting for classifier\n",
    "    clf = LGBMClassifier(random_state = 0)\n",
    "    train_index = data[data['TARGET'].notnull()].index\n",
    "    train_columns = data.drop('TARGET', axis = 1).columns\n",
    "\n",
    "    score = 1\n",
    "    new_columns = []\n",
    "    while score > .7:\n",
    "        train_columns = train_columns.drop(new_columns)\n",
    "        clf.fit(data.loc[train_index, train_columns], data.loc[train_index, 'TARGET'])\n",
    "        f_imp = pd.Series(clf.feature_importances_, index = train_columns)\n",
    "        score = roc_auc_score(data.loc[train_index, 'TARGET'], \n",
    "                              clf.predict_proba(data.loc[train_index, train_columns])[:, 1])\n",
    "        new_columns = f_imp[f_imp > 0].index\n",
    "\n",
    "    data.drop(train_columns, axis = 1, inplace = True)\n",
    "    print('After removing features not interesting for classifier there are {0:d} features'.format(data.shape[1]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_scores(df, num_folds, params, stratified = False, verbose = -1, \n",
    "              save_train_prediction = False, train_prediction_file_name = 'train_prediction.csv',\n",
    "              save_test_prediction = True, test_prediction_file_name = 'test_prediction.csv'):\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    clf = LGBMClassifier(**params)\n",
    "\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = 1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits = num_folds, shuffle = True, random_state = 1001)\n",
    "        \n",
    "    # Create arrays and dataframes to store results\n",
    "    train_pred = np.zeros(train_df.shape[0])\n",
    "    train_pred_proba = np.zeros(train_df.shape[0])\n",
    "\n",
    "    test_pred = np.zeros(train_df.shape[0])\n",
    "    test_pred_proba = np.zeros(train_df.shape[0])\n",
    "    \n",
    "    prediction = np.zeros(test_df.shape[0])\n",
    "    \n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    df_feature_importance = pd.DataFrame(index = feats)\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        print('Fold', n_fold, 'started at', time.ctime())\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        clf.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)], eval_metric = 'auc', \n",
    "                verbose = verbose, early_stopping_rounds = 200)\n",
    "\n",
    "        train_pred[train_idx] = clf.predict(train_x, num_iteration = clf.best_iteration_)\n",
    "        train_pred_proba[train_idx] = clf.predict_proba(train_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        test_pred[valid_idx] = clf.predict(valid_x, num_iteration = clf.best_iteration_)\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        \n",
    "        prediction += \\\n",
    "                clf.predict_proba(test_df[feats], num_iteration = clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        df_feature_importance[n_fold] = pd.Series(clf.feature_importances_, index = feats)\n",
    "        \n",
    "        print('Fold %2d AUC : %.6f' % (n_fold, roc_auc_score(valid_y, test_pred_proba[valid_idx])))\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    roc_auc_train = roc_auc_score(train_df['TARGET'], train_pred_proba)\n",
    "    precision_train = precision_score(train_df['TARGET'], train_pred, average = None)\n",
    "    recall_train = recall_score(train_df['TARGET'], train_pred, average = None)\n",
    "    \n",
    "    roc_auc_test = roc_auc_score(train_df['TARGET'], test_pred_proba)\n",
    "    precision_test = precision_score(train_df['TARGET'], test_pred, average = None)\n",
    "    recall_test = recall_score(train_df['TARGET'], test_pred, average = None)\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_test)\n",
    "    \n",
    "    df_feature_importance.fillna(0, inplace = True)\n",
    "    df_feature_importance['mean'] = df_feature_importance.mean(axis = 1)\n",
    "    \n",
    "    # Write prediction files\n",
    "    if save_train_prediction:\n",
    "        df_prediction = train_df[['SK_ID_CURR', 'TARGET']]\n",
    "        df_prediction['Prediction'] = test_pred_proba\n",
    "        df_prediction.to_csv(train_prediction_file_name, index = False)\n",
    "        del df_prediction\n",
    "        gc.collect()\n",
    "\n",
    "    if save_test_prediction:\n",
    "        df_prediction = test_df[['SK_ID_CURR']]\n",
    "        df_prediction['TARGET'] = prediction\n",
    "        df_prediction.to_csv(test_prediction_file_name, index = False)\n",
    "        del df_prediction\n",
    "        gc.collect()\n",
    "    \n",
    "    return df_feature_importance, \\\n",
    "           [roc_auc_train, roc_auc_test,\n",
    "            precision_train[0], precision_test[0], precision_train[1], precision_test[1],\n",
    "            recall_train[0], recall_test[0], recall_train[1], recall_test[1], 0]\n",
    "\n",
    "def display_folds_importances(feature_importance_df_, n_folds = 5):\n",
    "    n_columns = 3\n",
    "    n_rows = (n_folds + 1) // n_columns\n",
    "    _, axes = plt.subplots(n_rows, n_columns, figsize=(8 * n_columns, 8 * n_rows))\n",
    "    for i in range(n_folds):\n",
    "        sns.barplot(x = i, y = 'index', data = feature_importance_df_.reset_index().sort_values(i, ascending = False).head(20), \n",
    "                    ax = axes[i // n_columns, i % n_columns])\n",
    "    sns.barplot(x = 'mean', y = 'index', data = feature_importance_df_.reset_index().sort_values('mean', ascending = False).head(20), \n",
    "                    ax = axes[n_rows - 1, n_columns - 1])\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_evaluate(**params):\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "        \n",
    "    clf = LGBMClassifier(**params, n_estimators = 10000, nthread = 4)\n",
    "\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "\n",
    "    folds = KFold(n_splits = 2, shuffle = True, random_state = 1001)\n",
    "        \n",
    "    test_pred_proba = np.zeros(train_df.shape[0])\n",
    "    \n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        clf.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)], eval_metric = 'auc', \n",
    "                verbose = False, early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        \n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(train_df['TARGET'], test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "lgbm_params = {\n",
    "            'nthread': -1,\n",
    "            'n_estimators': 10000,\n",
    "            'learning_rate': .02,\n",
    "            'num_leaves': 34,\n",
    "            'colsample_bytree': .9497036,\n",
    "            'subsample': .8715623,\n",
    "            'max_depth': 8,\n",
    "            'reg_alpha': .041545473,\n",
    "            'reg_lambda': .0735294,\n",
    "            'min_split_gain': .0222415,\n",
    "            'min_child_weight': 39.3259775,\n",
    "            'silent': -1,\n",
    "            'verbose': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_evaluate(**params):\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "        \n",
    "    clf = LGBMClassifier(**params, n_estimators = 10000, nthread = 4)\n",
    "\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "\n",
    "    folds = KFold(n_splits = 2, shuffle = True, random_state = 1001)\n",
    "        \n",
    "    test_pred_proba = np.zeros(train_df.shape[0])\n",
    "    \n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        clf.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)], eval_metric = 'auc', \n",
    "                verbose = False, early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        \n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(train_df['TARGET'], test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing empty features there are 267 features\n",
      "After removing features with the same distribution on 0 and 1 classes there are 160 features\n",
      "After removing features with not the same distribution on train and test datasets there are 160 features\n",
      "After removing features not interesting for classifier there are 136 features\n",
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n",
      "    1 | 03m13s | \u001b[35m   0.76516\u001b[0m | \u001b[32m            0.9367\u001b[0m | \u001b[32m         0.0066\u001b[0m | \u001b[32m     8.2649\u001b[0m | \u001b[32m           37.6679\u001b[0m | \u001b[32m          0.0861\u001b[0m | \u001b[32m     38.1333\u001b[0m | \u001b[32m     0.0655\u001b[0m | \u001b[32m      0.0581\u001b[0m | \u001b[32m     0.6034\u001b[0m | \n",
      "    2 | 01m26s |    0.76444 |             0.9631 |          0.0194 |      6.5561 |            30.6284 |           0.0550 |      36.8829 |      0.0321 |       0.0807 |      0.9089 | \n",
      "    3 | 01m18s | \u001b[35m   0.76534\u001b[0m | \u001b[32m            0.8250\u001b[0m | \u001b[32m         0.0216\u001b[0m | \u001b[32m    18.2608\u001b[0m | \u001b[32m           32.7520\u001b[0m | \u001b[32m          0.0558\u001b[0m | \u001b[32m     27.4963\u001b[0m | \u001b[32m     0.0570\u001b[0m | \u001b[32m      0.0705\u001b[0m | \u001b[32m     0.7446\u001b[0m | \n",
      "    4 | 01m59s |    0.76517 |             0.8517 |          0.0105 |     15.5510 |            32.9932 |           0.0370 |      31.9843 |      0.0684 |       0.0865 |      0.6775 | \n",
      "    5 | 01m08s |    0.76503 |             0.7106 |          0.0261 |      6.7872 |            25.2807 |           0.0218 |      35.3063 |      0.0248 |       0.0406 |      0.8211 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n",
      "    6 | 01m33s | \u001b[35m   0.76588\u001b[0m | \u001b[32m            0.6262\u001b[0m | \u001b[32m         0.0269\u001b[0m | \u001b[32m     6.6332\u001b[0m | \u001b[32m           43.7246\u001b[0m | \u001b[32m          0.0720\u001b[0m | \u001b[32m     25.0199\u001b[0m | \u001b[32m     0.0601\u001b[0m | \u001b[32m      0.0904\u001b[0m | \u001b[32m     0.6114\u001b[0m | \n",
      "    7 | 02m21s | \u001b[35m   0.76615\u001b[0m | \u001b[32m            0.6227\u001b[0m | \u001b[32m         0.0114\u001b[0m | \u001b[32m    19.8458\u001b[0m | \u001b[32m           44.9449\u001b[0m | \u001b[32m          0.0157\u001b[0m | \u001b[32m     38.9301\u001b[0m | \u001b[32m     0.0516\u001b[0m | \u001b[32m      0.0571\u001b[0m | \u001b[32m     0.7438\u001b[0m | \n",
      "    8 | 01m21s |    0.76505 |             0.6620 |          0.0260 |     19.6861 |            20.3696 |           0.0809 |      39.8197 |      0.0669 |       0.0492 |      0.6076 | \n",
      "    9 | 03m36s | \u001b[35m   0.76617\u001b[0m | \u001b[32m            0.6049\u001b[0m | \u001b[32m         0.0075\u001b[0m | \u001b[32m    19.0153\u001b[0m | \u001b[32m           44.8373\u001b[0m | \u001b[32m          0.0794\u001b[0m | \u001b[32m     26.5521\u001b[0m | \u001b[32m     0.0474\u001b[0m | \u001b[32m      0.0508\u001b[0m | \u001b[32m     0.6665\u001b[0m | \n",
      "   10 | 02m45s |    0.76540 |             0.6177 |          0.0110 |      5.2339 |            21.2460 |           0.0926 |      25.0522 |      0.0328 |       0.0571 |      0.6254 | \n",
      "   11 | 11m29s |    0.76533 |             0.6301 |          0.0019 |      5.5549 |            44.9187 |           0.0828 |      39.6407 |      0.0518 |       0.0915 |      0.6821 | \n",
      "   12 | 01m34s |    0.76565 |             0.6047 |          0.0205 |     19.2252 |            36.5696 |           0.0128 |      39.5963 |      0.0355 |       0.0500 |      0.6449 | \n",
      "   13 | 01m53s |    0.76598 |             0.6036 |          0.0153 |     14.6345 |            43.9052 |           0.0239 |      25.4338 |      0.0623 |       0.0405 |      0.6253 | \n",
      "   14 | 01m26s |    0.76578 |             0.6514 |          0.0261 |     19.9601 |            44.8700 |           0.0234 |      31.4794 |      0.0644 |       0.0442 |      0.6670 | \n",
      "   15 | 02m42s |    0.76509 |             0.6949 |          0.0110 |      5.1729 |            20.0739 |           0.0844 |      38.9910 |      0.0512 |       0.0858 |      0.6658 | \n",
      "   16 | 01m49s |    0.76581 |             0.6096 |          0.0183 |     19.8911 |            22.5267 |           0.0887 |      25.5700 |      0.0591 |       0.0950 |      0.6048 | \n",
      "   17 | 01m59s |    0.76575 |             0.6061 |          0.0147 |      7.8815 |            35.2016 |           0.0993 |      26.6107 |      0.0723 |       0.0844 |      0.6399 | \n",
      "   18 | 09m52s |    0.76616 |             0.6171 |          0.0019 |     14.2030 |            44.2629 |           0.0958 |      36.3589 |      0.0246 |       0.0589 |      0.6130 | \n",
      "   19 | 01m22s |    0.76549 |             0.6159 |          0.0296 |     15.4236 |            44.5989 |           0.0946 |      39.5853 |      0.0693 |       0.0641 |      0.7319 | \n",
      "   20 | 09m27s |    0.76452 |             0.9761 |          0.0020 |     12.8756 |            28.5071 |           0.0301 |      25.0197 |      0.0258 |       0.0960 |      0.6089 | \n",
      "   21 | 05m08s |    0.76593 |             0.6018 |          0.0043 |     19.7008 |            38.4842 |           0.0589 |      25.1219 |      0.0349 |       0.0713 |      0.6383 | \n",
      "   22 | 04m59s |    0.76559 |             0.6043 |          0.0039 |     18.3356 |            20.2983 |           0.0827 |      29.3897 |      0.0268 |       0.0494 |      0.9554 | \n",
      "   23 | 02m06s |    0.76574 |             0.6060 |          0.0181 |      5.3113 |            44.3891 |           0.0881 |      31.9871 |      0.0319 |       0.0487 |      0.6894 | \n",
      "   24 | 05m07s |    0.76611 |             0.6562 |          0.0043 |     19.9541 |            44.7597 |           0.0904 |      38.9958 |      0.0355 |       0.0428 |      0.6199 | \n",
      "   25 | 14m30s |    0.76571 |             0.6015 |          0.0011 |     19.8126 |            26.1561 |           0.0946 |      34.4778 |      0.0401 |       0.0588 |      0.6538 | \n",
      "   26 | 02m51s |    0.76550 |             0.6229 |          0.0108 |      5.0006 |            35.6857 |           0.0279 |      30.2867 |      0.0283 |       0.0619 |      0.6041 | \n",
      "   27 | 09m12s |    0.76598 |             0.6557 |          0.0023 |     11.0285 |            43.6940 |           0.0857 |      29.7776 |      0.0295 |       0.0801 |      0.9984 | \n",
      "   28 | 08m14s |    0.76595 |             0.6049 |          0.0023 |      9.0349 |            34.5512 |           0.0947 |      39.7743 |      0.0700 |       0.0463 |      0.7073 | \n",
      "   29 | 01m41s |    0.76574 |             0.6000 |          0.0199 |     10.2260 |            38.1973 |           0.0994 |      35.3139 |      0.0678 |       0.0968 |      0.8457 | \n",
      "   30 | 07m53s |    0.76616 |             0.6020 |          0.0029 |     19.6039 |            44.9738 |           0.0597 |      26.1788 |      0.0263 |       0.0641 |      0.9597 | \n",
      "best params for  ../Input/app_only.csv\n",
      "{'colsample_bytree': 0.6048919785206204, 'learning_rate': 0.007461398833659209, 'num_leaves': 26, 'subsample': 0.6664881115064348, 'max_depth': 19, 'reg_alpha': 0.04744193504688145, 'reg_lambda': 0.05078450429338147, 'min_split_gain': 0.07943623165017753, 'min_child_weight': 44.83732551800324}\n",
      "Starting LightGBM. Train shape: (307500, 136), test shape: (48744, 136)\n",
      "Fold 0 started at Sat Aug 11 04:39:37 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[853]\ttraining's auc: 0.822996\tvalid_1's auc: 0.768399\n",
      "Fold  0 AUC : 0.768399\n",
      "Fold 1 started at Sat Aug 11 04:40:55 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1028]\ttraining's auc: 0.83171\tvalid_1's auc: 0.77173\n",
      "Fold  1 AUC : 0.771730\n",
      "Fold 2 started at Sat Aug 11 04:42:23 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[955]\ttraining's auc: 0.827651\tvalid_1's auc: 0.76521\n",
      "Fold  2 AUC : 0.765210\n",
      "Fold 3 started at Sat Aug 11 04:43:48 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1138]\ttraining's auc: 0.835702\tvalid_1's auc: 0.772617\n",
      "Fold  3 AUC : 0.772617\n",
      "Fold 4 started at Sat Aug 11 04:45:23 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1071]\ttraining's auc: 0.833603\tvalid_1's auc: 0.767933\n",
      "Fold  4 AUC : 0.767933\n",
      "Full AUC score 0.769153\n",
      "After removing empty features there are 2082 features\n",
      "After removing features with the same distribution on 0 and 1 classes there are 1345 features\n",
      "After removing features with not the same distribution on train and test datasets there are 1289 features\n",
      "After removing features not interesting for classifier there are 712 features\n",
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n",
      "    1 | 08m21s | \u001b[35m   0.77420\u001b[0m | \u001b[32m            0.7102\u001b[0m | \u001b[32m         0.0153\u001b[0m | \u001b[32m    10.7551\u001b[0m | \u001b[32m           20.0423\u001b[0m | \u001b[32m          0.0412\u001b[0m | \u001b[32m     26.5546\u001b[0m | \u001b[32m     0.0702\u001b[0m | \u001b[32m      0.0465\u001b[0m | \u001b[32m     0.8955\u001b[0m | \n",
      "    2 | 10m51s | \u001b[35m   0.77524\u001b[0m | \u001b[32m            0.7513\u001b[0m | \u001b[32m         0.0126\u001b[0m | \u001b[32m    19.4100\u001b[0m | \u001b[32m           44.6271\u001b[0m | \u001b[32m          0.0230\u001b[0m | \u001b[32m     31.4685\u001b[0m | \u001b[32m     0.0614\u001b[0m | \u001b[32m      0.0918\u001b[0m | \u001b[32m     0.8846\u001b[0m | \n",
      "    3 | 30m24s |    0.77518 |             0.8582 |          0.0048 |     19.2672 |            32.5756 |           0.0365 |      27.2931 |      0.0310 |       0.0745 |      0.7253 | \n",
      "    4 | 06m19s |    0.77458 |             0.6930 |          0.0206 |      6.5267 |            29.2991 |           0.0371 |      35.1284 |      0.0756 |       0.0480 |      0.7185 | \n",
      "    5 | 23m47s |    0.77453 |             0.8984 |          0.0053 |      9.1039 |            33.4640 |           0.0914 |      33.5769 |      0.0644 |       0.0573 |      0.7923 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n",
      "    6 | 87m24s |    0.77323 |             0.9766 |          0.0012 |     19.6351 |            20.4004 |           0.0500 |      39.9004 |      0.0465 |       0.0531 |      0.6399 | \n",
      "    7 | 07m22s | \u001b[35m   0.77588\u001b[0m | \u001b[32m            0.6264\u001b[0m | \u001b[32m         0.0193\u001b[0m | \u001b[32m     6.3169\u001b[0m | \u001b[32m           44.2137\u001b[0m | \u001b[32m          0.0409\u001b[0m | \u001b[32m     25.1083\u001b[0m | \u001b[32m     0.0275\u001b[0m | \u001b[32m      0.0615\u001b[0m | \u001b[32m     0.6159\u001b[0m | \n",
      "    8 | 07m01s |    0.77547 |             0.6358 |          0.0203 |     19.7794 |            43.5135 |           0.0156 |      25.8408 |      0.0773 |       0.0483 |      0.6259 | \n",
      "    9 | 13m54s |    0.77507 |             0.6258 |          0.0078 |      5.0384 |            44.5324 |           0.0308 |      39.0899 |      0.0582 |       0.0426 |      0.6177 | \n",
      "   10 | 17m58s |    0.77553 |             0.6098 |          0.0051 |      6.7058 |            44.9335 |           0.0426 |      26.6986 |      0.0498 |       0.0576 |      0.9955 | \n",
      "   11 | 06m09s |    0.77504 |             0.6337 |          0.0209 |     19.7428 |            37.2465 |           0.0852 |      39.8456 |      0.0341 |       0.0702 |      0.6155 | \n",
      "   12 | 11m44s |    0.77476 |             0.7232 |          0.0100 |      5.1608 |            39.6204 |           0.0245 |      25.8811 |      0.0668 |       0.0921 |      0.6691 | \n",
      "   13 | 12m19s |    0.77508 |             0.6028 |          0.0092 |     19.1687 |            22.1465 |           0.0811 |      30.7067 |      0.0269 |       0.0741 |      0.6018 | \n",
      "   14 | 08m48s |    0.77547 |             0.6179 |          0.0144 |     12.9827 |            42.0012 |           0.0107 |      31.6138 |      0.0222 |       0.0769 |      0.6037 | \n",
      "   15 | 06m54s |    0.77586 |             0.6155 |          0.0211 |     13.0184 |            44.5501 |           0.0912 |      25.1637 |      0.0239 |       0.0723 |      0.7455 | \n",
      "   16 | 07m01s |    0.77578 |             0.6337 |          0.0187 |      6.7766 |            44.6757 |           0.0824 |      28.1981 |      0.0228 |       0.0591 |      0.6102 | \n",
      "   17 | 07m58s |    0.77568 |             0.6554 |          0.0181 |      6.9910 |            44.4967 |           0.0608 |      25.3555 |      0.0742 |       0.0935 |      0.6057 | \n",
      "   18 | 12m59s |    0.77536 |             0.6992 |          0.0083 |      9.4625 |            44.2125 |           0.0273 |      25.0666 |      0.0224 |       0.0975 |      0.6206 | \n",
      "   19 | 05m38s |    0.77498 |             0.6076 |          0.0251 |     17.5819 |            44.8566 |           0.0296 |      39.4708 |      0.0629 |       0.0458 |      0.6205 | \n",
      "   20 | 05m14s |    0.77407 |             0.6178 |          0.0273 |      5.7563 |            20.9276 |           0.0759 |      39.2948 |      0.0311 |       0.0523 |      0.8315 | \n",
      "   21 | 05m47s |    0.77524 |             0.6143 |          0.0290 |     19.6927 |            37.9365 |           0.0914 |      30.9538 |      0.0317 |       0.0413 |      0.6133 | \n",
      "   22 | 14m26s |    0.77437 |             0.6768 |          0.0076 |     19.8064 |            20.0722 |           0.0926 |      25.2667 |      0.0659 |       0.0881 |      0.7639 | \n",
      "   23 | 06m38s |    0.77494 |             0.6009 |          0.0185 |     12.8213 |            30.5206 |           0.0943 |      39.7333 |      0.0224 |       0.0539 |      0.9549 | \n",
      "   24 | 05m42s |    0.77473 |             0.6127 |          0.0274 |     14.4481 |            27.5254 |           0.0757 |      25.3850 |      0.0420 |       0.0958 |      0.7062 | \n",
      "   25 | 08m57s |    0.77414 |             0.7614 |          0.0153 |      5.1751 |            21.1210 |           0.0867 |      25.3135 |      0.0217 |       0.0764 |      0.6008 | \n",
      "   26 | 08m41s |    0.77462 |             0.9994 |          0.0227 |     14.0912 |            44.6748 |           0.0829 |      28.9498 |      0.0621 |       0.0481 |      0.6041 | \n",
      "   27 | 12m09s |    0.77538 |             0.6169 |          0.0096 |     19.8535 |            27.5890 |           0.0594 |      30.2519 |      0.0431 |       0.0636 |      0.9913 | \n",
      "   28 | 14m52s |    0.77481 |             0.6513 |          0.0073 |      5.1722 |            36.8519 |           0.0723 |      39.5105 |      0.0258 |       0.0993 |      0.8264 | \n",
      "   29 | 06m12s |    0.77544 |             0.6111 |          0.0214 |     12.6820 |            44.8271 |           0.0927 |      36.0775 |      0.0238 |       0.0948 |      0.7745 | \n",
      "   30 | 16m36s | \u001b[35m   0.77605\u001b[0m | \u001b[32m            0.6126\u001b[0m | \u001b[32m         0.0073\u001b[0m | \u001b[32m    19.9067\u001b[0m | \u001b[32m           44.7539\u001b[0m | \u001b[32m          0.0965\u001b[0m | \u001b[32m     25.5178\u001b[0m | \u001b[32m     0.0385\u001b[0m | \u001b[32m      0.0580\u001b[0m | \u001b[32m     0.6320\u001b[0m | \n",
      "best params for  ../Input/app_burea_balance.csv\n",
      "{'colsample_bytree': 0.6126094266683103, 'learning_rate': 0.007296938991151308, 'num_leaves': 25, 'subsample': 0.6319925740518472, 'max_depth': 19, 'reg_alpha': 0.038535463541695236, 'reg_lambda': 0.058042106677596766, 'min_split_gain': 0.09646539183810851, 'min_child_weight': 44.753862288625484}\n",
      "Starting LightGBM. Train shape: (307500, 712), test shape: (48744, 712)\n",
      "Fold 0 started at Sat Aug 11 11:36:08 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[857]\ttraining's auc: 0.841679\tvalid_1's auc: 0.776801\n",
      "Fold  0 AUC : 0.776801\n",
      "Fold 1 started at Sat Aug 11 11:42:15 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1595]\ttraining's auc: 0.875861\tvalid_1's auc: 0.781396\n",
      "Fold  1 AUC : 0.781398\n",
      "Fold 2 started at Sat Aug 11 11:51:25 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1242]\ttraining's auc: 0.862614\tvalid_1's auc: 0.772647\n",
      "Fold  2 AUC : 0.772647\n",
      "Fold 3 started at Sat Aug 11 11:59:14 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1214]\ttraining's auc: 0.858846\tvalid_1's auc: 0.782601\n",
      "Fold  3 AUC : 0.782601\n",
      "Fold 4 started at Sat Aug 11 12:06:55 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1764]\ttraining's auc: 0.883562\tvalid_1's auc: 0.778305\n",
      "Fold  4 AUC : 0.778305\n",
      "Full AUC score 0.778281\n",
      "After removing empty features there are 1126 features\n",
      "After removing features with the same distribution on 0 and 1 classes there are 710 features\n",
      "After removing features with not the same distribution on train and test datasets there are 710 features\n",
      "After removing features not interesting for classifier there are 614 features\n",
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n",
      "    1 | 16m24s | \u001b[35m   0.77443\u001b[0m | \u001b[32m            0.8287\u001b[0m | \u001b[32m         0.0057\u001b[0m | \u001b[32m    15.4612\u001b[0m | \u001b[32m           30.0128\u001b[0m | \u001b[32m          0.0176\u001b[0m | \u001b[32m     30.7181\u001b[0m | \u001b[32m     0.0377\u001b[0m | \u001b[32m      0.0963\u001b[0m | \u001b[32m     0.7402\u001b[0m | \n",
      "    2 | 05m12s | \u001b[35m   0.77467\u001b[0m | \u001b[32m            0.7180\u001b[0m | \u001b[32m         0.0190\u001b[0m | \u001b[32m    13.6090\u001b[0m | \u001b[32m           36.6222\u001b[0m | \u001b[32m          0.0541\u001b[0m | \u001b[32m     37.8486\u001b[0m | \u001b[32m     0.0275\u001b[0m | \u001b[32m      0.0780\u001b[0m | \u001b[32m     0.8514\u001b[0m | \n",
      "    3 | 03m38s |    0.77379 |             0.7100 |          0.0291 |     11.3253 |            34.3734 |           0.0647 |      39.7806 |      0.0612 |       0.0526 |      0.7844 | \n",
      "    4 | 33m41s |    0.77450 |             0.9159 |          0.0032 |     14.3611 |            26.2762 |           0.0643 |      37.2986 |      0.0650 |       0.0445 |      0.9418 | \n",
      "    5 | 21m48s |    0.77381 |             0.9829 |          0.0043 |     14.2968 |            30.6580 |           0.0433 |      30.5586 |      0.0230 |       0.0594 |      0.7738 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n",
      "    6 | 50m34s | \u001b[35m   0.77506\u001b[0m | \u001b[32m            0.6000\u001b[0m | \u001b[32m         0.0010\u001b[0m | \u001b[32m    20.0000\u001b[0m | \u001b[32m           45.0000\u001b[0m | \u001b[32m          0.0100\u001b[0m | \u001b[32m     40.0000\u001b[0m | \u001b[32m     0.0200\u001b[0m | \u001b[32m      0.0400\u001b[0m | \u001b[32m     0.6000\u001b[0m | \n",
      "    7 | 15m01s | \u001b[35m   0.77531\u001b[0m | \u001b[32m            0.6216\u001b[0m | \u001b[32m         0.0049\u001b[0m | \u001b[32m    19.8359\u001b[0m | \u001b[32m           44.7599\u001b[0m | \u001b[32m          0.0684\u001b[0m | \u001b[32m     25.4662\u001b[0m | \u001b[32m     0.0793\u001b[0m | \u001b[32m      0.0598\u001b[0m | \u001b[32m     0.9559\u001b[0m | \n",
      "    8 | 08m13s |    0.77401 |             0.7302 |          0.0111 |     19.9283 |            20.7435 |           0.0862 |      25.9570 |      0.0545 |       0.0792 |      0.9793 | \n",
      "    9 | 05m15s |    0.77477 |             0.6562 |          0.0189 |     19.9655 |            29.4551 |           0.0834 |      39.3886 |      0.0365 |       0.0984 |      0.9464 | \n",
      "   10 | 03m51s |    0.77443 |             0.6034 |          0.0259 |      5.2543 |            20.2044 |           0.0214 |      27.7427 |      0.0235 |       0.0760 |      0.9809 | \n",
      "   11 | 06m56s |    0.77524 |             0.6200 |          0.0137 |      5.9063 |            44.8460 |           0.0240 |      27.3423 |      0.0369 |       0.0532 |      0.7379 | \n",
      "   12 | 04m09s |    0.77455 |             0.6011 |          0.0234 |     17.0157 |            20.1259 |           0.0874 |      37.8064 |      0.0288 |       0.0580 |      0.6899 | \n",
      "   13 | 06m13s |    0.77493 |             0.6227 |          0.0157 |      5.1335 |            44.8876 |           0.0116 |      37.4492 |      0.0217 |       0.0757 |      0.9897 | \n",
      "   14 | 03m44s |    0.77485 |             0.6164 |          0.0295 |     13.0504 |            44.6898 |           0.0579 |      31.9997 |      0.0609 |       0.0419 |      0.9970 | \n",
      "   15 | 38m16s |    0.77417 |             0.6321 |          0.0013 |      6.0241 |            20.0551 |           0.0300 |      38.1499 |      0.0483 |       0.0990 |      0.9491 | \n",
      "   16 | 13m20s |    0.77435 |             0.9880 |          0.0092 |     19.7864 |            44.6269 |           0.0140 |      34.7545 |      0.0793 |       0.0892 |      0.6822 | \n",
      "   17 | 32m28s |    0.77316 |             0.6361 |          0.0012 |      5.2825 |            36.2568 |           0.0186 |      25.2732 |      0.0421 |       0.0737 |      0.9983 | \n",
      "   18 | 05m16s |    0.77451 |             0.7740 |          0.0219 |     16.3593 |            44.5135 |           0.0190 |      25.3152 |      0.0308 |       0.0963 |      0.9584 | \n",
      "   19 | 04m33s |    0.77461 |             0.6018 |          0.0220 |     18.8300 |            35.9791 |           0.0237 |      35.0423 |      0.0305 |       0.0409 |      0.9969 | \n",
      "   20 | 27m34s | \u001b[35m   0.77532\u001b[0m | \u001b[32m            0.6092\u001b[0m | \u001b[32m         0.0025\u001b[0m | \u001b[32m    10.3676\u001b[0m | \u001b[32m           22.3089\u001b[0m | \u001b[32m          0.0944\u001b[0m | \u001b[32m     34.7665\u001b[0m | \u001b[32m     0.0221\u001b[0m | \u001b[32m      0.0971\u001b[0m | \u001b[32m     0.9843\u001b[0m | \n",
      "   21 | 30m16s | \u001b[35m   0.77556\u001b[0m | \u001b[32m            0.6062\u001b[0m | \u001b[32m         0.0022\u001b[0m | \u001b[32m    14.9679\u001b[0m | \u001b[32m           44.8755\u001b[0m | \u001b[32m          0.0879\u001b[0m | \u001b[32m     39.9380\u001b[0m | \u001b[32m     0.0671\u001b[0m | \u001b[32m      0.0766\u001b[0m | \u001b[32m     0.9741\u001b[0m | \n",
      "   22 | 20m14s |    0.77506 |             0.6275 |          0.0034 |     18.1320 |            27.1967 |           0.0978 |      34.7469 |      0.0307 |       0.0819 |      0.6071 | \n",
      "   23 | 06m31s |    0.77473 |             0.6121 |          0.0127 |     11.9348 |            20.1072 |           0.0137 |      25.0880 |      0.0552 |       0.0957 |      0.6293 | \n",
      "   24 | 24m30s |    0.77491 |             0.6548 |          0.0030 |     15.0336 |            20.2015 |           0.0550 |      30.4923 |      0.0576 |       0.0872 |      0.9799 | \n",
      "   25 | 08m39s |    0.77508 |             0.6179 |          0.0089 |      5.1449 |            44.9756 |           0.0885 |      33.2527 |      0.0277 |       0.0910 |      0.9348 | \n",
      "   26 | 17m41s |    0.77549 |             0.6066 |          0.0041 |      8.9709 |            44.9411 |           0.0201 |      36.2320 |      0.0729 |       0.0651 |      0.6083 | \n",
      "   27 | 05m26s |    0.77497 |             0.6112 |          0.0180 |     15.1490 |            44.8880 |           0.0701 |      35.4471 |      0.0335 |       0.0976 |      0.7238 | \n",
      "   28 | 04m35s |    0.77451 |             0.6283 |          0.0227 |      9.6730 |            20.9257 |           0.0130 |      32.9767 |      0.0624 |       0.0960 |      0.6409 | \n",
      "   29 | 16m32s |    0.77501 |             0.6427 |          0.0047 |     19.1224 |            27.6629 |           0.0392 |      25.0305 |      0.0451 |       0.0965 |      0.9522 | \n",
      "   30 | 17m18s | \u001b[35m   0.77557\u001b[0m | \u001b[32m            0.6045\u001b[0m | \u001b[32m         0.0042\u001b[0m | \u001b[32m     9.3634\u001b[0m | \u001b[32m           43.9094\u001b[0m | \u001b[32m          0.0231\u001b[0m | \u001b[32m     35.7389\u001b[0m | \u001b[32m     0.0425\u001b[0m | \u001b[32m      0.0994\u001b[0m | \u001b[32m     0.9947\u001b[0m | \n",
      "best params for  ../Input/app_previous.csv\n",
      "{'colsample_bytree': 0.6044630541933609, 'learning_rate': 0.004153134679166414, 'num_leaves': 35, 'subsample': 0.9946911662433262, 'max_depth': 9, 'reg_alpha': 0.042534993685832384, 'reg_lambda': 0.0993539629482855, 'min_split_gain': 0.023145952484421223, 'min_child_weight': 43.90943860599516}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (307500, 614), test shape: (48744, 614)\n",
      "Fold 0 started at Sat Aug 11 20:00:29 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1202]\ttraining's auc: 0.863745\tvalid_1's auc: 0.77883\n",
      "Fold  0 AUC : 0.778830\n",
      "Fold 1 started at Sat Aug 11 20:07:04 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1096]\ttraining's auc: 0.859728\tvalid_1's auc: 0.781387\n",
      "Fold  1 AUC : 0.781387\n",
      "Fold 2 started at Sat Aug 11 20:13:10 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1672]\ttraining's auc: 0.885699\tvalid_1's auc: 0.775671\n",
      "Fold  2 AUC : 0.775671\n",
      "Fold 3 started at Sat Aug 11 20:21:25 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1545]\ttraining's auc: 0.880287\tvalid_1's auc: 0.781152\n",
      "Fold  3 AUC : 0.781152\n",
      "Fold 4 started at Sat Aug 11 20:29:20 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[953]\ttraining's auc: 0.851132\tvalid_1's auc: 0.777605\n",
      "Fold  4 AUC : 0.777605\n",
      "Full AUC score 0.778890\n",
      "After removing empty features there are 324 features\n",
      "After removing features with the same distribution on 0 and 1 classes there are 201 features\n",
      "After removing features with not the same distribution on train and test datasets there are 201 features\n",
      "After removing features not interesting for classifier there are 162 features\n",
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n",
      "    1 | 08m02s | \u001b[35m   0.77153\u001b[0m | \u001b[32m            0.9996\u001b[0m | \u001b[32m         0.0036\u001b[0m | \u001b[32m    17.8161\u001b[0m | \u001b[32m           44.0465\u001b[0m | \u001b[32m          0.0155\u001b[0m | \u001b[32m     36.4531\u001b[0m | \u001b[32m     0.0525\u001b[0m | \u001b[32m      0.0816\u001b[0m | \u001b[32m     0.9720\u001b[0m | \n",
      "    2 | 01m30s | \u001b[35m   0.77179\u001b[0m | \u001b[32m            0.7572\u001b[0m | \u001b[32m         0.0212\u001b[0m | \u001b[32m    17.2368\u001b[0m | \u001b[32m           34.4024\u001b[0m | \u001b[32m          0.0577\u001b[0m | \u001b[32m     27.6223\u001b[0m | \u001b[32m     0.0632\u001b[0m | \u001b[32m      0.0808\u001b[0m | \u001b[32m     0.7703\u001b[0m | \n",
      "    3 | 01m20s |    0.77111 |             0.9101 |          0.0266 |     17.4422 |            28.4585 |           0.0216 |      31.0384 |      0.0377 |       0.0969 |      0.7026 | \n",
      "    4 | 01m28s |    0.77116 |             0.9939 |          0.0242 |     19.2405 |            34.3527 |           0.0114 |      32.8892 |      0.0334 |       0.0831 |      0.7082 | \n",
      "    5 | 17m52s | \u001b[35m   0.77185\u001b[0m | \u001b[32m            0.6838\u001b[0m | \u001b[32m         0.0010\u001b[0m | \u001b[32m    15.4818\u001b[0m | \u001b[32m           37.6755\u001b[0m | \u001b[32m          0.0556\u001b[0m | \u001b[32m     39.7379\u001b[0m | \u001b[32m     0.0299\u001b[0m | \u001b[32m      0.0788\u001b[0m | \u001b[32m     0.6606\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n",
      "    6 | 03m23s |    0.77168 |             0.9205 |          0.0123 |      5.1695 |            44.4925 |           0.0394 |      25.8896 |      0.0335 |       0.0930 |      0.6301 | \n",
      "    7 | 12m10s | \u001b[35m   0.77292\u001b[0m | \u001b[32m            0.6039\u001b[0m | \u001b[32m         0.0023\u001b[0m | \u001b[32m    19.1523\u001b[0m | \u001b[32m           44.8429\u001b[0m | \u001b[32m          0.0795\u001b[0m | \u001b[32m     25.3178\u001b[0m | \u001b[32m     0.0378\u001b[0m | \u001b[32m      0.0585\u001b[0m | \u001b[32m     0.6111\u001b[0m | \n",
      "    8 | 05m48s |    0.77275 |             0.6137 |          0.0051 |     19.9837 |            44.7011 |           0.0111 |      26.0967 |      0.0657 |       0.0558 |      0.9722 | \n",
      "    9 | 01m54s |    0.77230 |             0.6018 |          0.0200 |     19.6372 |            44.7966 |           0.0188 |      27.2339 |      0.0496 |       0.0624 |      0.8710 | \n",
      "   10 | 02m25s |    0.77118 |             0.9764 |          0.0191 |      5.3046 |            20.1307 |           0.0990 |      39.6918 |      0.0477 |       0.0742 |      0.9116 | \n",
      "   11 | 02m55s |    0.77187 |             0.8924 |          0.0153 |      5.0880 |            44.9889 |           0.0368 |      39.9713 |      0.0635 |       0.0422 |      0.8580 | \n",
      "   12 | 01m53s |    0.77072 |             0.9082 |          0.0192 |     19.9052 |            20.3319 |           0.0957 |      25.0350 |      0.0421 |       0.0942 |      0.6123 | \n",
      "   13 | 01m44s |    0.77054 |             0.9468 |          0.0230 |     18.7711 |            20.1114 |           0.0515 |      39.9830 |      0.0320 |       0.0536 |      0.9887 | \n",
      "   14 | 01m53s |    0.77134 |             0.9979 |          0.0260 |     16.5462 |            44.9553 |           0.0249 |      25.1522 |      0.0420 |       0.0641 |      0.9841 | \n",
      "   15 | 04m48s |    0.77183 |             0.7176 |          0.0066 |      5.2544 |            20.2868 |           0.0129 |      25.0348 |      0.0208 |       0.0686 |      0.6069 | \n",
      "   16 | 01m47s |    0.77192 |             0.6400 |          0.0218 |     19.7485 |            44.4393 |           0.0970 |      39.8519 |      0.0692 |       0.0892 |      0.8841 | \n",
      "   17 | 04m32s |    0.77197 |             0.8206 |          0.0081 |      5.0708 |            32.7128 |           0.0863 |      25.0236 |      0.0498 |       0.0505 |      0.7070 | \n",
      "   18 | 02m41s |    0.77236 |             0.6954 |          0.0144 |     19.4733 |            42.6203 |           0.0149 |      25.3702 |      0.0794 |       0.0521 |      0.6212 | \n",
      "   19 | 03m27s |    0.77228 |             0.6106 |          0.0098 |      5.0515 |            36.6206 |           0.0968 |      33.5762 |      0.0343 |       0.0505 |      0.9928 | \n",
      "   20 | 03m57s |    0.77248 |             0.6334 |          0.0086 |     19.9043 |            36.5267 |           0.0805 |      25.2189 |      0.0287 |       0.0653 |      0.9840 | \n",
      "   21 | 06m08s |    0.77231 |             0.6247 |          0.0048 |      6.8096 |            21.6005 |           0.0844 |      30.5065 |      0.0739 |       0.0985 |      0.9760 | \n",
      "   22 | 03m08s |    0.77239 |             0.6153 |          0.0127 |      5.0013 |            29.9584 |           0.0759 |      39.7408 |      0.0690 |       0.0842 |      0.9318 | \n",
      "   23 | 11m23s |    0.77215 |             0.8261 |          0.0026 |     19.9963 |            44.7982 |           0.0887 |      25.7627 |      0.0689 |       0.0856 |      0.8547 | \n",
      "   24 | 13m08s |    0.77035 |             0.6022 |          0.0012 |      5.4836 |            27.7803 |           0.0770 |      35.4530 |      0.0713 |       0.0404 |      0.7281 | \n",
      "   25 | 02m23s |    0.77171 |             0.6300 |          0.0139 |     11.2914 |            20.8036 |           0.0934 |      25.1478 |      0.0461 |       0.0778 |      0.9723 | \n",
      "   26 | 15m41s |    0.77181 |             0.6443 |          0.0013 |     19.6078 |            20.4704 |           0.0323 |      32.1371 |      0.0596 |       0.0978 |      0.9091 | \n",
      "   27 | 04m49s |    0.77229 |             0.6292 |          0.0072 |      5.4702 |            38.4644 |           0.0125 |      39.4139 |      0.0313 |       0.0962 |      0.9186 | \n",
      "   28 | 03m23s |    0.77231 |             0.6038 |          0.0087 |     19.1670 |            29.4070 |           0.0545 |      39.7904 |      0.0774 |       0.0563 |      0.9934 | \n",
      "   29 | 04m25s |    0.77262 |             0.6089 |          0.0064 |     10.1570 |            44.9995 |           0.0954 |      37.7111 |      0.0548 |       0.0946 |      0.9286 | \n",
      "   30 | 01m57s |    0.77245 |             0.6165 |          0.0238 |      5.0638 |            38.5272 |           0.0824 |      25.1816 |      0.0627 |       0.0964 |      0.9789 | \n",
      "best params for  ../Input/app_cash.csv\n",
      "{'colsample_bytree': 0.603874946200733, 'learning_rate': 0.0022754647476639365, 'num_leaves': 25, 'subsample': 0.6110624897497796, 'max_depth': 19, 'reg_alpha': 0.03778117987279202, 'reg_lambda': 0.05849036728519236, 'min_split_gain': 0.07946015422614841, 'min_child_weight': 44.84287565889986}\n",
      "Starting LightGBM. Train shape: (307500, 162), test shape: (48744, 162)\n",
      "Fold 0 started at Sat Aug 11 23:08:35 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[997]\ttraining's auc: 0.84082\tvalid_1's auc: 0.774642\n",
      "Fold  0 AUC : 0.774642\n",
      "Fold 1 started at Sat Aug 11 23:10:12 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1287]\ttraining's auc: 0.8548\tvalid_1's auc: 0.779336\n",
      "Fold  1 AUC : 0.779336\n",
      "Fold 2 started at Sat Aug 11 23:12:10 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1180]\ttraining's auc: 0.850061\tvalid_1's auc: 0.77245\n",
      "Fold  2 AUC : 0.772450\n",
      "Fold 3 started at Sat Aug 11 23:14:03 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1400]\ttraining's auc: 0.859278\tvalid_1's auc: 0.778225\n",
      "Fold  3 AUC : 0.778225\n",
      "Fold 4 started at Sat Aug 11 23:16:11 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1187]\ttraining's auc: 0.849133\tvalid_1's auc: 0.773797\n",
      "Fold  4 AUC : 0.773797\n",
      "Full AUC score 0.775658\n",
      "After removing empty features there are 343 features\n",
      "After removing features with the same distribution on 0 and 1 classes there are 226 features\n",
      "After removing features with not the same distribution on train and test datasets there are 226 features\n",
      "After removing features not interesting for classifier there are 172 features\n",
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n",
      "    1 | 05m03s | \u001b[35m   0.77540\u001b[0m | \u001b[32m            0.8887\u001b[0m | \u001b[32m         0.0077\u001b[0m | \u001b[32m    13.5015\u001b[0m | \u001b[32m           41.8682\u001b[0m | \u001b[32m          0.0245\u001b[0m | \u001b[32m     26.7133\u001b[0m | \u001b[32m     0.0630\u001b[0m | \u001b[32m      0.0600\u001b[0m | \u001b[32m     0.6651\u001b[0m | \n",
      "    2 | 01m34s |    0.77455 |             0.8991 |          0.0258 |     14.9697 |            27.1535 |           0.0751 |      26.1697 |      0.0692 |       0.0666 |      0.7417 | \n"
     ]
    }
   ],
   "source": [
    "input_files = ['../Input/app_only.csv',\n",
    "              '../Input/app_burea_balance.csv',\n",
    "              '../Input/app_previous.csv',\n",
    "              '../Input/app_cash.csv',\n",
    "              '../Input/app_installments.csv',\n",
    "              '../Input/app_credit_card_balance.csv']\n",
    "\n",
    "output_files = ['../Submissions/app_only_cleaned_optimized.csv',\n",
    "               '../Submissions/app_bureau_balanced_cleaned_optimized.csv',\n",
    "               '../Submissions/app_previous_cleaned_optimized.csv',\n",
    "               '../Submissions/app_cash_cleaned_optimized.csv',\n",
    "               '../Submissions/app_installments_cleaned_optimized',\n",
    "               '../Submissions/app_credit_card_balance_cleaned_optimized.csv']\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    df = pd.read_csv(input_files[i])\n",
    "    df = clean_data(df)\n",
    "    \n",
    "    params = {'colsample_bytree': (0.6, 1),\n",
    "          'learning_rate': (.001, .03), \n",
    "          'num_leaves': (25, 40), \n",
    "          'subsample': (0.6, 1), \n",
    "          'max_depth': (5, 20), \n",
    "          'reg_alpha': (.02, .08), \n",
    "          'reg_lambda': (.04, .1), \n",
    "          'min_split_gain': (.01, .1),\n",
    "          'min_child_weight': (20, 45)}\n",
    "    \n",
    "    \n",
    "    bo = BayesianOptimization(lgbm_evaluate, params)\n",
    "    bo.maximize(init_points = 5, n_iter = 25)\n",
    "    best_params = bo.res['max']['max_params']\n",
    "    best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    bo.res['max']['max_val']\n",
    "    print('best params for ', input_files[i])\n",
    "    print(best_params)\n",
    "    \n",
    "    feature_importance, scor = cv_scores(df, 5, lgbm_params, test_prediction_file_name = output_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
